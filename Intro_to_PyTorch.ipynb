{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks and PyTorch\n",
    "---\n",
    "### Outline\n",
    "* Setup\n",
    "* Main Components\n",
    "    * Neural Networks Architecture\n",
    "        * Multi-Layers Perceptron\n",
    "            * Linear Layers (a.k.a fully connected layers)\n",
    "            * Nonlinear Activation Layers\n",
    "            * Loss function\n",
    "        * Convolutional Neural Networks (brief introduction)\n",
    "    * Gradients Calculation with Backpropagation\n",
    "    * Parameters Optimization\n",
    "        * Gradient Descent\n",
    "        * Storchastic Gradient Descent\n",
    "* Current and Future Reserach \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "In practice, the following dependencies are not needed for this tutorial if you have immediate skills with computer programming. However, to reduce the complexity of this tutorial and take advantages of the speed up provided by parrralalizing and the GPU, we will rely on the folling tools. To learn how to code it with out any of the following dependencies, you can following this [video tutorial](https://www.youtube.com/watch?v=hfMk-kjRv4c).\n",
    "\n",
    "* **PyTorch** (For creating tensors of various sizes, calculating gradient analytically and performing parameters optimization)\n",
    "* **matplotlib** (Plotting tool)\n",
    "* **pandas** (Holding data for plotting)\n",
    "\n",
    "[Optional] Start by installing [Anaconda](https://www.anaconda.com/products/distribution) or [miniconda](https://docs.conda.io/en/latest/miniconda.html#latest-miniconda-installer-links) to create python enviorment. Otherwise, replace the command `conda` with `pip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "# Create conda enviorment (optional)\n",
    "conda create --name pydemo python=3.8\n",
    "\n",
    "conda activate pydemo\n",
    "\n",
    "# Pytorch: CPU version (recommendation: use `www.pytorch.org` to choose the best PyTorch version for you)\n",
    "conda install pytorch cpuonly -c pytorch\n",
    "\n",
    "# Pytorch: Nvidia GPU version (recommendation: use `www.pytorch.org` to choose the best PyTorch version for you)\n",
    "conda install pytorch cudatoolkit=11.6 -c pytorch -c conda-forge\n",
    "\n",
    "conda install matplotlib seaborn -c conda-forge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup: *Random Dataset*\n",
    "\n",
    "The following code is to generate an random dataset with 2 inputs and 1 label for demonstrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndata = 400\n",
    "layer_n = [2, 3, 3, 1]\n",
    "\n",
    "## data generation\n",
    "X = th.rand(ndata, layer_n[0])\n",
    "Y = 1.0*(th.linalg.norm(X-th.tensor([0.1, 0.1]), dim=1) > 0.5)\n",
    "X_grid, Y_grid = th.meshgrid(\n",
    "                th.linspace(0, 1, steps=100), \n",
    "                th.linspace(0, 1, steps=100), indexing='ij')\n",
    "grid_in = th.stack((X_grid, Y_grid), 2).view(-1, 2)\n",
    "\n",
    "## store data for plotting\n",
    "dtpd = pd.DataFrame(X, columns=['$x_1$', '$x_2$'])\n",
    "dtpd['y'] = Y\n",
    "Y = Y.unsqueeze(dim=-1)\n",
    "dtpd['label_color'] = dtpd['y'].apply(lambda y: \"red\" if y==1 else \"blue\")\n",
    "\n",
    "## plotting\n",
    "plt.scatter(data=dtpd, x='$x_1$', y='$x_2$', c=dtpd['label_color'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and plot functions\n",
    "\n",
    "def train(net, opt, criterion, epochs=10000, train_size=-1, batch_size=-1):\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(X[:train_size])\n",
    "        loss = criterion(outputs, Y[:train_size])\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # log loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return losses\n",
    "\n",
    "def get_heatmap(net):\n",
    "    heat = net(grid_in).view(100, 100)\n",
    "    pred = net(X)\n",
    "    return heat, pred\n",
    "\n",
    "def plot_boundary_loss(fig, heat, losses, title=\"Test\"):\n",
    "    heat[heat > 0.51] = 1.0\n",
    "    heat[heat < 0.49] = 0.0\n",
    "    heat = heat.cpu().detach().numpy()\n",
    "\n",
    "    subfgs = fig.subfigures(1, 2)\n",
    "    ax1 = subfgs[0].subplots(1, 1)\n",
    "    ax2 = subfgs[1].subplots(1, 1)\n",
    "\n",
    "    ax1.imshow(heat, cmap='bwr', interpolation='bicubic', alpha=0.4, origin='lower')\n",
    "    ax1.set_title('Boundary line')\n",
    "    ax1.scatter(dtpd['$x_1$']*99, dtpd['$x_2$']*99, c=dtpd['label_color'])\n",
    "    ax1.set_xlim([0, 99])\n",
    "    ax1.set_ylim([0, 99])\n",
    "\n",
    "    # plt.figure()\n",
    "    ax2.set_title('Loss over Epochs')\n",
    "    ax2.plot(losses)\n",
    "    ax2.set_xlabel('epoch')\n",
    "    ax2.set_ylabel('MSE')\n",
    "    ax2.grid()\n",
    "\n",
    "    fig.suptitle(title, fontsize=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks Architecture\n",
    "\n",
    "Let $\\hat{X}$ represent an input domain, $\\hat{Y}$ represent an output codomain and $\\hat{f}: \\hat{X} \\rightarrow \\hat{Y}$ an unknown function that maps values from the set $\\hat{X}$ to $\\hat{Y}$. Given subsets $X \\subseteq \\hat{X}$ and $Y \\subseteq \\hat{Y}$, can we build a function $f$ that equals $\\hat{f}$?\n",
    "\n",
    "Recents works on *Universal Approximation Theorem* establishes the capabilities of artificial neural networks (in particular Multi-Layer Perceptron) to approximate the mapping between two euclidian spaces given a big enough error of margin $\\epsilon > 0$ and number of neurons satisfying, $$\\int_{X}\\|f(x)-\\hat{f}(x)\\|dx < \\epsilon$$ There is some works discussing the approximation capabilities of neural networks for non-euclidian spaces but the work is ongoing. For further reading, seen footnote below.\n",
    "\n",
    "<center><img src=\"imgs\\approximation_classes.png\" width=\"500\"/></center>\n",
    "\n",
    "*Reference:* \n",
    "* [Universal Approximation Theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem) (Wikipedia)\n",
    "* [A Universal Approximation Theorem of Deep Neural Networks for Expressing Probability Distributions](https://proceedings.neurips.cc/paper/2020/hash/2000f6325dfc4fc3201fc45ed01c7a5d-Abstract.html), Lu, Yulong 2020\n",
    "* [Neural network approximation](https://www.cambridge.org/core/services/aop-cambridge-core/content/view/7077A90FB36D405D903DCC82683B7A48/S0962492921000052a.pdf/neural-network-approximation.pdf), DeVore 2021\n",
    "* [Deep Neural Network Approximation Theory](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9363169), Elbr√§chter 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks Architecture: *Multi-Layer Perceptron (MLP)*\n",
    "\n",
    "Multilayer perceptron (or fully-connected neural networks) are a sequence of repeating linear tranformations followed by non-linear transformation for which the linear transform matrixed $W_i$ are learniable parameters.\n",
    "\n",
    "<center><img src=\"imgs/nn_structure.bmp\" width=\"500\"/></center>\n",
    "\n",
    "For example, the 3-layers MLP illustrated above can presented the sequence of tranformations:\n",
    "\n",
    "$$z_1= W_1 x + b_1\\\\\n",
    "a_1=\\sigma(z_1)\\\\\n",
    "z_2= W_2 a_1 + b_2\\\\\n",
    "a_2=\\sigma(z_2)\\\\\n",
    "z_3= W_3 a_2 + b_3\\\\\n",
    "h=\\sigma(z_3)$$\n",
    "\n",
    "Which can be condenced to the following equations for batch of inputs and associated outputs:\n",
    "\n",
    "$$\\bar{z}_1= \\bar{x}\\bar{W_1}^{T}\\\\\n",
    "\\bar{a}_1=\\sigma(\\bar{z}_1)\\\\\n",
    "\\bar{z}_2= \\bar{a}_1\\bar{W_2}^{T}\\\\\n",
    "\\bar{a}_2=\\sigma(\\bar{z}_2)\\\\\n",
    "\\bar{z}_3= \\bar{a}_2\\bar{W_3}^{T}\\\\\n",
    "\\bar{h}=\\sigma(\\bar{z}_3)$$\n",
    "\n",
    "Where , $\\sigma$ is a non-linear function (e.g. sigmoid function, ReLu, etc.) and\n",
    "\n",
    "$$\\bar{x} = \\begin{bmatrix} x_{b1}^T & 1\\\\ x_{b2}^T & 1\\\\ \\vdots & \\vdots \\\\ x_{bk}^T & 1\\end{bmatrix}, \n",
    "\\bar{W_i} = \\begin{bmatrix} W_i & b_i\\end{bmatrix}$$\n",
    "\n",
    "such that $\\bar{W}_i \\in R^{n\\times (m+1)}$ are learnable parameters, $x_{bi}\\in \\hat{X} \\subseteq R^m$, $n$ is the size of the output vector and $m$ is the size of the input vector and k is the size of input batch.\n",
    "\n",
    "### A MLP without Non-Linear Activation Layers\n",
    "\n",
    "Let $x_i \\in X \\subseteq \\hat{X} = \\{x: 0 \\leq x \\leq 1, x \\in R^2 \\}$, $y_i \\in Y  \\subseteq \\hat{Y} = \\{0, 1\\}$ and $\\hat{f}: \\hat{X} \\rightarrow \\hat{Y}$ an unknown function. Then, let $f_{MLP}(x)$ be a neural network that approximates $\\hat{f}$ defined as,\n",
    "\n",
    "$$f_{MLP}(x) = \\sigma(\\bar{x}\\bar{W_1}^{T}\\bar{W_2}^{T}\\bar{W_3}^{T}) = \\sigma(\\bar{x}\\bar{W}_{1,2,3}^{T})$$\n",
    "\n",
    "Where $\\sigma(x) = \\frac{1}{1+e^{-x}}$ (sigmoid function), $W_1 \\in R^{10\\times 2}$, $W_2 \\in R^{10\\times 10}$ , $W_1 \\in R^{1\\times 10}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MLP without non-linear activation layers\n",
    "\n",
    "# Construct model\n",
    "layers = [nn.Linear(layer_n[i], layer_n[i+1]) for i in range(len(layer_n)-1)]\n",
    "layers.append(nn.Sigmoid())\n",
    "\n",
    "model = nn.Sequential(*layers)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = th.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Calculate Gradient and Optimize\n",
    "losses = train(model, optimizer, criterion)\n",
    "\n",
    "# Plot boundary line and loss\n",
    "heat, pred = get_heatmap(model)\n",
    "fig = plt.figure()\n",
    "plot_boundary_loss(fig, heat, losses, title=\"MLP without non-linear functions\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Linear Activation Layers\n",
    "\n",
    "As seen in the MLP consisting of just linear activation, a neural network with just linear transformation is not able to create non-linear boundary lines that may separate a given dataset optimally. Thus, we must include non-linear functions to create non-linear boundary lines. \n",
    "\n",
    "Let $f_{MLP}(x)$ be a neural network that approximates $\\hat{f}$ defined as,\n",
    "\n",
    "$$f_{MLP}(x) = \\sigma(\\sigma(\\sigma(\\bar{x}\\bar{W_1}^{T})\\bar{W_2}^{T})\\bar{W_3}^{T})$$\n",
    "\n",
    "Where $W_1 \\in R^{10\\times 2}$, $W_2 \\in R^{10\\times 10}$, $W_1 \\in R^{1\\times 10}$ and\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}} \\text{ (sigmoid function)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xline = th.linspace(-15, 15, steps=100)\n",
    "plt.axhline(0, color='grey')\n",
    "plt.axvline(0, color='grey')\n",
    "plt.plot(xline, 1/(1+th.exp(xline)), label=\"$\\sigma(x)=\\\\frac{1}{1+e^{-x}}$\", c='blue')\n",
    "plt.title(\"Sigmoid Activation\")\n",
    "plt.legend(loc=1)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MLP with sigmoid function\n",
    "\n",
    "# Construct model\n",
    "layers = []\n",
    "for i in range(len(layer_n)-1):\n",
    "    layers.append(nn.Linear(layer_n[i], layer_n[i+1]))\n",
    "    layers.append(nn.Sigmoid())\n",
    "\n",
    "model = nn.Sequential(*layers)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = th.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "# Calculate Gradient and Optimize\n",
    "losses = train(model, optimizer, criterion, epochs=15000)\n",
    "\n",
    "# Plot boundary line and loss\n",
    "heat, pred = get_heatmap(model)\n",
    "fig = plt.figure()\n",
    "plot_boundary_loss(fig, heat, losses, title=\"MLP with Sigmoid functions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for all hidden layers (that is all layers except for the output layer or all layers extracting features from input), lets define,\n",
    "\n",
    "$$\\sigma(x) = \\max (0, x) \\text{ (ReLu function)}$$\n",
    "\n",
    "For all hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xline = th.linspace(-15, 15, steps=100)\n",
    "plt.axhline(0, color='grey')\n",
    "plt.axvline(0, color='grey')\n",
    "plt.plot(xline, th.relu(xline), label=\"$\\sigma(x)=\\\\max(0, x)$\", c='blue')\n",
    "plt.legend(loc=1)\n",
    "plt.title(\"ReLu Activation\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MLP with Relu function\n",
    "\n",
    "# Construct model\n",
    "layers = [nn.Linear(layer_n[0], layer_n[1])]\n",
    "for i in range(1,len(layer_n)-1):\n",
    "    layers.append(nn.ReLU())\n",
    "    layers.append(nn.Linear(layer_n[i], layer_n[i+1]))\n",
    "layers.append(nn.Sigmoid())\n",
    "\n",
    "model = nn.Sequential(*layers)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = th.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Calculate Gradient and Optimize\n",
    "losses = train(model, optimizer, criterion, epochs=15000)\n",
    "\n",
    "# Plot boundary line and loss\n",
    "heat, pred = get_heatmap(model)\n",
    "fig = plt.figure()\n",
    "plot_boundary_loss(fig, heat, losses, title=\"MLP with ReLu functions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "As aforementioned, the goal with a neural network is to define a function $f(x)$ such that \n",
    "\n",
    "$$\\int_{X}\\|f(x)-\\hat{f}(x)\\|dx < \\epsilon$$ \n",
    "\n",
    "for the smallest $\\epsilon > 0$ possible. As such, we must optimize our neural network so that $\\hat{f}_{obj}(X) = \\int_{X}\\|f(x)-\\hat{f}(x)\\|dx$ is as small as possible. However, it has been found that optimizing for $\\hat{f}_{obj}(X)$ directly may at times take a significant amount of time and resources. As such, researchers have proposed various objective function alternatives $f_{obj}(X)$ known as loss functions that based on a given unknown function $\\hat{f}(x)$, they may converge faster to a function $f(x)$ that has a lower $\\epsilon$ than if one was to optimize for $\\hat{f}_{obj}(X)$ directly. There are instances that $\\hat{Y} \\subseteq Y$ may also not be known. Common loss functions include:\n",
    "\n",
    "1. **MAE, MSE, etc.**\n",
    "    $$f_{obj}(X,Y) = \\sum^{N}_i \\frac{ \\| f(x_i)- y_i \\|}{N} $$\n",
    "2. **MBE**\n",
    "    $$f_{obj}(X,Y) = \\sum^{N}_i \\frac{ f(x_i)- y_i}{N}$$\n",
    "3. **Cross Entropy Loss/Negative Log Likelihood**\n",
    "    $$f_{obj}(X,Y) = -\\sum^{N}_i [f(x_i) log(y_i) + (1-f(x_i)) log(1 - y_i)]$$\n",
    "4. **Heat Equation**\n",
    "    $$f_{obj}(X) = \\sum^{N}_i \\frac{\\partial f}{\\partial \\bar{x}_t} - c^2 \\frac{\\partial f}{\\partial \\bar{x}_x} \\bigg\\rvert_{\\bar{x}=x_i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks Architecture: *Convolutional Neural Network (CNN)*\n",
    "\n",
    "Although it is not appropriate for the classification task at hand, there is another popular nueral network architecture that we should reference. That is the convolutional neural network. It relies on the convolution operation which is defined as,\n",
    "\n",
    "$$(f \\ast g)(t):=\\int_{t_0}^{t_f} f(t-\\tau) g(\\tau) d \\tau$$\n",
    "\n",
    "is the continous domain, and, more appropriate for us, as,\n",
    "\n",
    "$$(f \\ast g)[n]:= \\sum_{k=k_0}^{k_f} f[n-k] g[k]$$\n",
    "\n",
    "in the discrite domain where [$t_0$, $t_f$] and [$k_0$, $k_f$] is the support of $g(t)$ and $g[k]$, respectively, and $f(t)$ and $f[k]$ are expected to have a larger than or equal to support as $g(t)$ and $g[k]$, respectively. Note that the fourier and laplace tranforms are application of convolution in which the kernel $g(t)$ is define as $e^{-st}$ and $e^{-jwt}$, respectivetly, with free variables s and frequency w. Also, note that these transform hold special properties for convolutions which will not be discussed in this tutorial. Below are a vizualizations of 1D and 2D convolutions, respectively.\n",
    "\n",
    "    1D Convolution\n",
    "<center><img src=\"imgs\\1D_Convolution_continous.gif\" width=\"500\"/> </center>\n",
    "<center><img src=\"imgs\\1D_Convolution.gif\" height=\"300\"/> </center>\n",
    "\n",
    "    2D Convolution\n",
    "<center><img src=\"imgs\\2D_Convolution.gif\" height=\"300\"/></center>\n",
    "\n",
    "Now, the idea with CNN is to make $g[k]$ an discritized tensor $\\tilde{g}[k]$ that is a learnable parameter like $W$ (aforementioned during MLP discussion). Then, inserting the convolution operation into an existing neural network structure. For example, an typical CNN would look something like,\n",
    "\n",
    "$$z_1 = (f \\ast \\tilde{g})[\\tilde{X}]\\\\\n",
    "a_1 = \\sigma(z_1)\\\\\n",
    "z_2 = (f \\ast \\tilde{g})[a_1]\\\\\n",
    "a_2 = \\sigma(z_2)\\\\\n",
    "a^*_2 = vector(a_2)\\\\\n",
    "h = f_{MLP}(a^*_2)$$\n",
    "\n",
    "where $(f \\ast g)[\\tilde{X}]$ is the convolution operation of every element of the discritized input $f$ in the domain $\\tilde{X}$ (think of an image), $f_{MLP}(x)$ is an MLP as described before, and $vector(a)$ is the operation of flattening the input tensor $a \\in R^{n \\times \\dots \\times n}$ into a vector $\\hat{a} \\in R^{n^k}$ where k is the number of dimensions of $a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients Calculation with Backpropagation\n",
    "\n",
    "Although it is not necessary to optimize the paramaters of a neural network (such as when using evolution based optimization), many optimization schemes use gradients to train neural networks. As such, the backpropagation algorithm was developed to automatically compute the gradients of operations in the neural network and loss function with respect to various variables and parameters. In paritcular, most optimization scheme rely on calculation the greadient of the loss (i.e. object) with repespect to the learnable parameters and using that information to update the model for teh next training iteration. It relies on the chain rule of calculus to avoid recomputing gradients, and solves for the gradients analytacially by tracking the operations performed and storing the known gradient formulation. To explore backpropagation, we will use PyTorch autograd tool and look at the following example.\n",
    "\n",
    "Given $x_i \\in X \\subseteq \\hat{X}$ and $y_i \\in Y \\subseteq \\hat{Y}$, lets define a generic neural network and a generic associated loss function,\n",
    "\n",
    "$$f_{MLP}(x)=\\sigma(W_2 \\sigma(W_1 x + b_1) + b_2)$$\n",
    "\n",
    "$$f_{obj}(x_i,y_i) = (f_{MLP}(x_i)- y_i)^2 $$\n",
    "\n",
    "These set of operations can further be discomposed into sequence of simpler operations,\n",
    "\n",
    "$$\n",
    "z_1= W_1 x_i + b_1\\\\\n",
    "a_1=\\sigma(z_1)\\\\\n",
    "z_2= W_2 a_1 + b_2\\\\\n",
    "h=\\sigma(z_2)\\\\\n",
    "L = (h - y_i)^2\n",
    "$$\n",
    "\n",
    "Now, the goal with this backpropagation algorithm is to calculate the gradient of the loss function $L$ with respect to each of the learnable parameter by starting with the loss and calculating the gradient backwards from the loss to each parameter. For example, to calculate the gradient $\\frac{\\partial L}{\\partial W_1}$ and $\\frac{\\partial L}{\\partial b_2}$, we can discompose them using the chain rule the following ways,\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W_1} = \\textcolor{grey}{\\frac{\\partial L}{\\partial h} \\frac{\\partial h}{\\partial z_2}} \\frac{\\partial z_2}{\\partial a_1} \\frac{\\partial a_1}{\\partial z_1}  \\frac{\\partial z_1}{\\partial W_1}$$\n",
    "$$\\frac{\\partial L}{\\partial b_2} = \\textcolor{grey}{\\frac{\\partial L}{\\partial h} \\frac{\\partial h}{\\partial z_2}} \\frac{\\partial z_2}{\\partial b_2}$$\n",
    "\n",
    "As seen, we can take advantage of the fact that many partial derivatives (examples highlighted in $\\textcolor{grey}{grey}$) of the gradients of the loss are shared and thus, for a given input, one can store these partial derivatives and reuse them. We can also observe that to calculate the gradients of loss $L$ with respect to an earlier learnable parameters in the order of operations, we must first compute the gradient of the loss with respect to that parameter's function out. Hence, the algorithm name, 'backpropagation' as we must first work backwards to compute the partial derivative of the later operations before we can calculate the earlier ones and propagate the results back.\n",
    "\n",
    "PyTorch's autograd and others' backpropagation algorithms additionally keeps an record of the analytical partial derivative fomulation of each simplier operation with respect to each of the parameters (learnable and unlearnable ones). This allows the tool to calculate the exact result for each individual gradient. Common partitial derivative formulas include:\n",
    "\n",
    "$$\\frac{\\partial (Wx+b)}{\\partial W} = x $$\n",
    "\n",
    "$$\\frac{\\partial (Wx+b)}{\\partial x} = W $$\n",
    "\n",
    "$$\\frac{\\partial (Wx+b)}{\\partial b} = 1 $$\n",
    "\n",
    "$$\\frac{\\partial \\sigma}{\\partial x} = \\frac{e^x}{1+e^{x}} = \\sigma(x)(1-\\sigma(x)) \\text{ (Sigmoid)}$$\n",
    "\n",
    "$$\\frac{\\partial \\sigma}{\\partial x} = \\begin{cases} \n",
    "0 & \\text{if  }  x \\leq 0 \\\\\n",
    "1 & \\text{if  }  x > 0 \\\\\n",
    "\\end{cases} \\text{ (ReLu)}$$\n",
    "\n",
    "$$\\frac{\\partial ((h - y_i)^2)}{\\partial h} = 2h $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of pytorch backwards and autograd\n",
    "\n",
    "x = th.rand(2, requires_grad=True).T\n",
    "W = th.rand(2, 2, requires_grad=True)\n",
    "b = th.rand(2, requires_grad=True).T\n",
    "y = th.ones(2)\n",
    "\n",
    "z = W*x + b\n",
    "h = th.sigmoid(z)\n",
    "loss = h - y\n",
    "\n",
    "## Method 1: backwards()\n",
    "loss.backward() # calculate all gradient with respect to loss\n",
    "print(W.grad)\n",
    "\n",
    "## Method 2: gradient of h with respect to x\n",
    "d_h_dx = th.autograd.grad(outputs=h, inputs=x)[0]\n",
    "print(d_h_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learnable Parameter Optimization\n",
    "\n",
    "#### Gradient Decent\n",
    "\n",
    "#### Storchastic Gradient Decent\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient decent\n",
    "\n",
    "epochs = 6000\n",
    "train_size = -1\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    opt.zero_grad()\n",
    "\n",
    "    outputs = net(X[:train_size])\n",
    "    loss = criterion(outputs, Y[:train_size])\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    # log loss\n",
    "    losses.append(loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pydemo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c9d8fb51efedf0439e01e3b02b35dbcefa59a21a44576dda94af83c5e332bc24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
